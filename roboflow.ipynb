{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# Config Loader\n",
    "# -------------------------------\n",
    "class Config:\n",
    "    def __init__(self, config_path=\"CNN_Object_Detection/configs/config.yml\"):\n",
    "        with open(config_path, \"r\") as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "\n",
    "    def get(self, key, default=None):\n",
    "        keys = key.split(\".\")\n",
    "        value = self.config\n",
    "        for k in keys:\n",
    "            value = value.get(k, {})\n",
    "            if not isinstance(value, dict):\n",
    "                return value\n",
    "        return default\n",
    "\n",
    "# -------------------------------\n",
    "# CNN 모델\n",
    "# -------------------------------\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 640, 640), num_classes=1):\n",
    "        super(CNN, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.3), nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self.bbox_regressor = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim != 4 or tuple(x.shape[1:]) != self.input_shape:\n",
    "            raise ValueError(f\"Expected input shape [B, {self.input_shape}], but got {x.shape}\")\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        class_logits = self.classifier(x)\n",
    "        bbox = torch.sigmoid(self.bbox_regressor(x))\n",
    "        return class_logits, bbox\n",
    "\n",
    "# -------------------------------\n",
    "# YOLO Dataset\n",
    "# -------------------------------\n",
    "class YoloFolderDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, input_shape):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.input_shape = input_shape\n",
    "        self.image_files = sorted(os.listdir(img_dir))\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(input_shape[1:]),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        label_file = img_file.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\")\n",
    "        img_path = os.path.join(self.img_dir, img_file)\n",
    "        label_path = os.path.join(self.label_dir, label_file)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img)\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            parts = f.readline().strip().split()\n",
    "            class_id = int(parts[0])\n",
    "\n",
    "            # YOLO polygon 형식 -> AABB로 변환\n",
    "            poly_xys = [float(x) for x in parts[3:]]  # x1 y1 x2 y2 ...\n",
    "            xs = poly_xys[::2]\n",
    "            ys = poly_xys[1::2]\n",
    "\n",
    "            xmin = min(xs)\n",
    "            xmax = max(xs)\n",
    "            ymin = min(ys)\n",
    "            ymax = max(ys)\n",
    "\n",
    "            x_center = (xmin + xmax) / 2\n",
    "            y_center = (ymin + ymax) / 2\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "\n",
    "            bbox = torch.tensor([x_center, y_center, width, height], dtype=torch.float32)\n",
    "\n",
    "        return img_tensor, torch.tensor(class_id), bbox, img.copy(), img_file\n",
    "\n",
    "# -------------------------------\n",
    "# Custom collate_fn\n",
    "# -------------------------------\n",
    "def collate_fn(batch):\n",
    "    images, classes, bboxes, orig_imgs, img_names = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(images),\n",
    "        torch.tensor(classes),\n",
    "        torch.stack(bboxes),\n",
    "        orig_imgs,\n",
    "        img_names\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Training\n",
    "# -------------------------------\n",
    "def train(model, loader, optimizer, criterion_cls, criterion_bbox, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img, cls, bbox, _, _ in loader:\n",
    "        img, cls, bbox = img.to(device), cls.to(device), bbox.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out_cls, out_bbox = model(img)\n",
    "        loss_cls = criterion_cls(out_cls, cls)\n",
    "        loss_bbox = criterion_bbox(out_bbox, bbox)\n",
    "        loss = loss_cls + loss_bbox * 10.0  # bbox loss에 가중치 부여\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# -------------------------------\n",
    "# Inference + Visualization\n",
    "# -------------------------------\n",
    "def infer_and_visualize(model, loader, device, output_dir=\"predicted\", num_samples=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (img, cls, bbox, orig_imgs, img_names) in enumerate(loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            img = img.to(device)\n",
    "            out_cls, out_bbox = model(img)\n",
    "            pred_class = torch.argmax(out_cls, dim=1).item()\n",
    "            pred_bbox = out_bbox[0].cpu().numpy()\n",
    "\n",
    "            print(f\"[{img_names[0]}] ➤ Predicted bbox: {pred_bbox}\")\n",
    "\n",
    "            orig_img = orig_imgs[0]\n",
    "            img_name = img_names[0]\n",
    "            draw = ImageDraw.Draw(orig_img)\n",
    "            w, h = orig_img.size\n",
    "            cx, cy, bw, bh = pred_bbox\n",
    "            x1 = int((cx - bw / 2) * w)\n",
    "            y1 = int((cy - bh / 2) * h)\n",
    "            x2 = int((cx + bw / 2) * w)\n",
    "            y2 = int((cy + bh / 2) * h)\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "            draw.text((x1, y1), f\"Class {pred_class}\", fill=\"red\")\n",
    "            orig_img.save(os.path.join(output_dir, img_name))\n",
    "            print(f\"Saved: {os.path.join(output_dir, img_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "config = Config(\"CNN_Object_Detection/configs/config.yml\")\n",
    "input_shape = tuple(config.get(\"model.input_shape\"))\n",
    "num_classes = config.get(\"model.num_classes\")\n",
    "batch_size = config.get(\"train.batch_size\")\n",
    "epochs = config.get(\"train.epochs\")\n",
    "lr = config.get(\"train.learning_rate\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN(input_shape=input_shape, num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_bbox = nn.SmoothL1Loss()\n",
    "\n",
    "base_path = \"/home/kar/Projects/CNN_Object_Detection/Tank-detection-using-YOLO-8\"\n",
    "train_ds = YoloFolderDataset(f\"{base_path}/train/images\", f\"{base_path}/train/labels\", input_shape)\n",
    "test_ds  = YoloFolderDataset(f\"{base_path}/test/images\",  f\"{base_path}/test/labels\",  input_shape)\n",
    "valid_ds  = YoloFolderDataset(f\"{base_path}/valid/images\",  f\"{base_path}/valid/labels\",  input_shape)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader  = DataLoader(valid_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", class_names=None, plot_confusion=False):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    print(device)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, cls, _, _, _ in loader:\n",
    "            img = img.to(device)\n",
    "            cls = cls.to(device)\n",
    "            out_cls, _ = model(img)\n",
    "            preds = torch.argmax(out_cls, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(cls.cpu().numpy())\n",
    "\n",
    "    unique_labels = sorted(list(set(all_labels)))\n",
    "    if class_names is None:\n",
    "        class_names = [f\"class_{i}\" for i in unique_labels]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds, target_names=class_names, zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"\\nClassification Evaluation\")\n",
    "    print(f\"Precision : {precision:.4f}\")\n",
    "    print(f\"Recall    : {recall:.4f}\")\n",
    "    print(f\"F1-Score  : {f1:.4f}\")\n",
    "    print(\"\\n\" + report)\n",
    "\n",
    "    # confusion matrix\n",
    "    if plot_confusion:\n",
    "        cm = confusion_matrix(all_labels, all_preds, labels=unique_labels)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path=\"model.pth\"):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"모델 저장 완료: {path}\")\n",
    "\n",
    "def load_model(model_class, path, input_shape, num_classes, device):\n",
    "    model = model_class(input_shape=input_shape, num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"모델 로드 완료: {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/10 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m save_model(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 133\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion_cls, criterion_bbox, device)\u001b[0m\n\u001b[1;32m    131\u001b[0m loss_bbox \u001b[38;5;241m=\u001b[39m criterion_bbox(out_bbox, bbox)\n\u001b[1;32m    132\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_cls \u001b[38;5;241m+\u001b[39m loss_bbox \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10.0\u001b[39m  \u001b[38;5;66;03m# bbox loss에 가중치 부여\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    135\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "    loss = train(model, train_loader, optimizer, criterion_cls, criterion_bbox, device)\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {loss:.4f}\")\n",
    "\n",
    "save_model(model, \"model.pth\")\n",
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optimizer_state\": optimizer.state_dict()\n",
    "}, \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and infer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드 완료: model.pth\n",
      "\n",
      "Inference + Visualization...\n",
      "cuda\n",
      "\n",
      "Classification Evaluation\n",
      "Precision : 1.0000\n",
      "Recall    : 1.0000\n",
      "F1-Score  : 1.0000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       1.00      1.00      1.00        83\n",
      "\n",
      "    accuracy                           1.00        83\n",
      "   macro avg       1.00      1.00      1.00        83\n",
      "weighted avg       1.00      1.00      1.00        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(CNN, \"model.pth\", input_shape, num_classes, device)\n",
    "checkpoint = torch.load(\"checkpoint.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "\n",
    "print(\"\\nInference + Visualization...\")\n",
    "# infer_and_visualize(model, test_loader, device, output_dir=\"predicted\", num_samples=20)\n",
    "evaluate_model(model, valid_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_inference_speed(model, loader, device, warmup=5, num_batches=50):\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    total_images = 0\n",
    "\n",
    "    # GPU warm-up\n",
    "    with torch.no_grad():\n",
    "        for i, (img, _, _, _, _) in enumerate(loader):\n",
    "            if i >= warmup:\n",
    "                break\n",
    "            img = img.to(device)\n",
    "            _ = model(img)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        for i, (img, _, _, _, _) in enumerate(loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            img = img.to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            _ = model(img)\n",
    "            end_time = time.time()\n",
    "\n",
    "            total_time += (end_time - start_time)\n",
    "            total_images += img.size(0)\n",
    "\n",
    "    fps = total_images / total_time if total_time > 0 else 0\n",
    "    print(f\"Inference Speed: {fps:.2f} FPS ({1000/fps:.2f} ms/frame)\")\n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Speed: 1088.81 FPS (0.92 ms/frame)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1088.8130876542632"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = \"/home/kar/Projects/CNN_Object_Detection/Tank-detection-using-YOLO-8\"\n",
    "test_ds  = YoloFolderDataset(f\"{base_path}/test/images\",  f\"{base_path}/test/labels\",  input_shape)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "measure_inference_speed(model, test_loader, device, warmup=5, num_batches=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ONNX export 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13089/985231872.py:49: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.ndim != 4 or tuple(x.shape[1:]) != self.input_shape:\n"
     ]
    }
   ],
   "source": [
    "odel = model.to(\"cpu\")\n",
    "dummy_input = torch.randn(1, 1, 640, 640)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"improved_cnn.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"class_logits\", \"bbox\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}},\n",
    "    opset_version=11\n",
    ")\n",
    "print(\"✅ ONNX export 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_13089/1329950183.py\", line 2, in <module>\n",
      "    import onnxruntime as ort\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/onnxruntime/__init__.py\", line 23, in <module>\n",
      "    from onnxruntime.capi._pybind_state import ExecutionMode  # noqa: F401\n",
      "  File \"/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/onnxruntime/capi/_pybind_state.py\", line 32, in <module>\n",
      "    from .onnxruntime_pybind11_state import *  # noqa\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mort\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmeasure_inference_speed_onnx\u001b[39m(session, loader, warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, num_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n",
      "File \u001b[0;32m~/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/onnxruntime/__init__.py:57\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m onnxruntime_validation\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m import_capi_exception:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m import_capi_exception\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime_inference_collection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceSession  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime_inference_collection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IOBinding  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/onnxruntime/__init__.py:23\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# we need to do device version validation (for example to check Cuda version for an onnxruntime-training package).\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# in order to know whether the onnxruntime package is for training it needs\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# to do import onnxruntime.training.ortmodule first.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# meaningful messages to the user.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# the saved exception is raised after device version validation.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pybind_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExecutionMode  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pybind_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExecutionOrder  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pybind_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphOptimizationLevel  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/onnxruntime/capi/_pybind_state.py:32\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(system_root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem32\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvcruntime140_1.dll\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m     25\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install the 2019 Visual C++ runtime and then try again. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve installed the runtime in a non-standard location \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(other than \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSystemRoot\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSystem32), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure it can be found by setting the correct path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime_pybind11_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# import onnxruntime as ort\n",
    "# import numpy as np\n",
    "\n",
    "def measure_inference_speed_onnx(session, loader, warmup=5, num_batches=50):\n",
    "    total_time = 0\n",
    "    total_images = 0\n",
    "    input_name = session.get_inputs()[0].name\n",
    "\n",
    "    # warm-up\n",
    "    for i, (img, _, _, _, _) in enumerate(loader):\n",
    "        if i >= warmup:\n",
    "            break\n",
    "        session.run(None, {input_name: img.numpy()})\n",
    "\n",
    "    # measure\n",
    "    for i, (img, _, _, _, _) in enumerate(loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "        start = time.time()\n",
    "        session.run(None, {input_name: img.numpy()})\n",
    "        end = time.time()\n",
    "\n",
    "        total_time += (end - start)\n",
    "        total_images += img.size(0)\n",
    "\n",
    "    fps = total_images / total_time\n",
    "    print(f\"⚡ Inference Speed (ONNX): {fps:.2f} FPS ({1000/fps:.2f} ms/frame)\")\n",
    "    return fps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model_onnx(session, loader, class_names=None, plot_confusion=False):\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, cls, _, _, _ in loader:\n",
    "            logits, _ = session.run(None, {input_name: img.numpy()})\n",
    "            preds = np.argmax(logits, axis=1)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(cls.numpy())\n",
    "\n",
    "    unique_labels = sorted(list(set(all_labels)))\n",
    "    if class_names is None:\n",
    "        class_names = [f\"class_{i}\" for i in unique_labels]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0)\n",
    "\n",
    "    print(\"\\n📊 ONNX Classification Evaluation\")\n",
    "    print(f\"Precision : {precision:.4f}\")\n",
    "    print(f\"Recall    : {recall:.4f}\")\n",
    "    print(f\"F1-Score  : {f1:.4f}\")\n",
    "    print(\"\\n\" + report)\n",
    "\n",
    "    if plot_confusion:\n",
    "        cm = confusion_matrix(all_labels, all_preds, labels=unique_labels)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"🧩 ONNX Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "\n",
      "📊 ONNX Classification Evaluation\n",
      "Precision : 1.0000\n",
      "Recall    : 1.0000\n",
      "F1-Score  : 1.0000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        tank       1.00      1.00      1.00        50\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "⚡ Inference Speed (ONNX): 43.15 FPS (23.18 ms/frame)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43.14508803984285"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import onnxruntime as ort\n",
    "\n",
    "# GPU 세션\n",
    "session = ort.InferenceSession(\"improved_cnn.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "\n",
    "import onnxruntime as ort\n",
    "print(ort.get_available_providers())\n",
    "\n",
    "# 평가\n",
    "evaluate_model_onnx(session, test_loader, class_names=[\"tank\"])\n",
    "measure_inference_speed_onnx(session, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar/Projects/CNN_Object_Detection/venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "\n",
      "📊 ONNX Classification Evaluation\n",
      "Precision : 1.0000\n",
      "Recall    : 1.0000\n",
      "F1-Score  : 1.0000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        tank       1.00      1.00      1.00        50\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "⚡ Inference Speed (ONNX): 40.28 FPS (24.83 ms/frame)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.28092806524384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import onnxruntime as ort\n",
    "\n",
    "# GPU 세션\n",
    "session = ort.InferenceSession(\"improved_cnn.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "\n",
    "import onnxruntime as ort\n",
    "print(ort.get_available_providers())\n",
    "\n",
    "# 평가\n",
    "evaluate_model_onnx(session, test_loader, class_names=[\"tank\"])\n",
    "measure_inference_speed_onnx(session, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
